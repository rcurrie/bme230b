{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying cancer expression vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will train a neural network to identify the tissue type that produced an RNA expression vector. The dataset is comprised of RNA-seq data obtained from tumors. \n",
    "\n",
    "For a complete description of the data collection workflow see this page:\n",
    "https://xenabrowser.net/datapages/?host=https://toil.xenahubs.net\n",
    "\n",
    "And for the corresponding publication:\n",
    "https://doi.org/10.1038/nbt.3772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing training data\n",
    "For this problem, expression data needs to be loaded and pruned. Initially, there are >50,000 genes in each expression vector, which can be reduced to a much smaller gene set for the sake of minimizing computation time. Here, the data is subsetted to only include genes from the KEGG gene set. You may want to consider reducing or expanding this dataset to get a better understanding of which genes are predictive, though this is not a requirement for the assignment.\n",
    "\n",
    "For a list of gene sets, check out the MSigDB collections page: http://software.broadinstitute.org/gsea/msigdb/collections.jsp\n",
    "\n",
    "This script was adapted from Rob Currie's ingestion script: https://github.com/rcurrie/tumornormal/blob/master/genesets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hdf files...\n",
      "Loaded 186 gene sets\n",
      "Subsetting to 5172 genes\n",
      "x_pruned shape:  19126 5172\n",
      "Training set x dimensions:  (15300, 5172)\n",
      "Training set y dimensions:  (15300, 46)\n",
      "Testing set x dimensions:  (3826, 5172)\n",
      "Testing set y dimensions:  (3826, 46)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading hdf files...\")\n",
    "\n",
    "tcga_h5_pathname = \"data/tcga_target_gtex_train.h5\"\n",
    "msigdb_gmt_pathname = \"data/c2.cp.kegg.v6.1.symbols.gmt\"    # Needs a login to download from URL. Must be hosted locally\n",
    "\n",
    "x = pd.read_hdf(tcga_h5_pathname, \"expression\")\n",
    "x.head()\n",
    "\n",
    "y = pd.read_hdf(tcga_h5_pathname, \"labels\")\n",
    "y.head()\n",
    "\n",
    "# Generate a set object from all of the TCGA gene symbols\n",
    "tcga_gene_set = set(x.columns.values)\n",
    "\n",
    "gene_sets = dict()\n",
    "\n",
    "# Load gene sets from downloaded MSigDB gmt file (KEGG gene sets)\n",
    "with open(msigdb_gmt_pathname) as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split('\\t')\n",
    "        set_name = line[1]\n",
    "        kegg_gene_subset = set(line[2:])\n",
    "\n",
    "        # Find genes that are in both the KEGG database AND the TCGA dataset (AKA the intersection)\n",
    "        genes = kegg_gene_subset & tcga_gene_set\n",
    "\n",
    "        # Store the sets in their separate categories (just in case we want to subset by category later)\n",
    "        gene_sets[set_name] = genes\n",
    "\n",
    "\n",
    "print(\"Loaded %d gene sets\" % len(gene_sets))\n",
    "\n",
    "# Find the union of all genes in the gene sets\n",
    "all_gene_set_genes = sorted(list(set.union(*[gene_set for gene_set in gene_sets.values()])))\n",
    "\n",
    "print(\"Subsetting to %d genes\" % len(all_gene_set_genes))\n",
    "\n",
    "# Prune x so that it only contains the genes that are in both TCGA and KEGG\n",
    "x_pruned = x.drop(labels=(set(x.columns)-set(all_gene_set_genes)), axis=1, errors=\"ignore\")\n",
    "\n",
    "assert x_pruned[\"TP53\"][\"TCGA-ZP-A9D4-01\"] == x[\"TP53\"][\"TCGA-ZP-A9D4-01\"]\n",
    "\n",
    "m,n = x_pruned.shape                # m = number of training examples\n",
    "print(\"x_pruned shape: \", m, n)\n",
    "\n",
    "# Make sure the genes are the same and in the same order\n",
    "assert len(all_gene_set_genes) == len(x_pruned.columns.values)\n",
    "assert list(x_pruned.columns.values) == all_gene_set_genes\n",
    "\n",
    "x_array = np.array(x_pruned.values, dtype=np.float32)\n",
    "\n",
    "# extract primary site names from data table\n",
    "y_names = list(set(y[\"primary_site\"].values))\n",
    "y_names = sorted(y_names)\n",
    "\n",
    "y_array = np.zeros(shape=(m, len(y_names)), dtype=np.float32)\n",
    "\n",
    "# create a key for converting names to indices\n",
    "y_index_key = {name:i for i,name in enumerate(y_names)}\n",
    "\n",
    "# generate one-hot vectors for all primary site y data\n",
    "for m,primary_site_name in enumerate(y[\"primary_site\"].values):\n",
    "    index = y_index_key[primary_site_name]\n",
    "    y_array[m,index] = 1\n",
    "\n",
    "# split data into training and testing set with equal class representation\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(x_array, y_array):\n",
    "    x_train, x_test = x_array[train_index], x_array[test_index]\n",
    "    y_train, y_test = y_array[train_index], y_array[test_index]\n",
    "\n",
    "print(\"Training set x dimensions: \", x_train.shape)\n",
    "print(\"Training set y dimensions: \", y_train.shape)\n",
    "print(\"Testing set x dimensions: \", x_test.shape)\n",
    "print(\"Testing set y dimensions: \", y_test.shape)\n",
    "\n",
    "# ensure that the output directory exists\n",
    "if not os.path.exists(\"data/\"):\n",
    "    os.mkdir(\"data/\")\n",
    "\n",
    "# save the data in compressed numpy files\n",
    "np.savez_compressed(\"data/x_train.npz\", a=x_train)\n",
    "np.savez_compressed(\"data/y_train.npz\", a=y_train)\n",
    "np.savez_compressed(\"data/x_test.npz\", a=x_test)\n",
    "np.savez_compressed(\"data/y_test.npz\", a=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our data has been stored in numpy format, which conveniently pytorch has a method for converting to their native format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimarySiteDataset(Dataset):\n",
    "    def __init__(self, x_path, y_path, batch_size=None):\n",
    "        x = np.load(x_path)['a']\n",
    "        y = np.load(y_path)['a']\n",
    "\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor     # for MSE Loss\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}